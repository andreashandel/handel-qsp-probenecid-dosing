# Review notes for analysis-code R scripts

## run-model-fitting.R
- **Best-fit output stores the wrong fixed parameters.** Inside `eval_one_sample()` the code fits using `fixedpars_i`, but the object returned to `bestfit_all` overwrites `bestfit$fixedpars` with the baseline `fixedpars` vector (line 379). If any Latin Hypercube samples are used, downstream scripts will pull the baseline settings rather than the values actually used during the fit. That breaks the intent of saving multiple calibrated parameter sets for later simulation and causes `simulate_dose_predictions()` to run with mismatched parameters. Please store `fixedpars_i` in the result instead.
- **Initial value adjustment hides boundary problems.** Lines 258–269 silently clamp the starting point inside the bounds. If a starting guess is out of range it would be safer to crash so the issue can be fixed explicitly, rather than quietly replacing the value. Consider dropping the adjustment and letting the run fail.

## dose-predictions-simulator-function.R
- **Implicit dependency on `here()`.** The helper script calls `source(here("code/analysis-code/model-simulator-function.R"))` (line 12) but never attaches the `here` package. It only works when the caller already loaded `here`, so sourcing the file directly (e.g., from tests) will error. Either add `library(here)` here or refer to `here::here()` explicitly.
- **Failed simulations are silently accepted.** Within `simulate_dose_response()` failures from `simulate_model()` are swallowed by `try(...)` and the loop just continues (lines 75–79). That leaves `summary_df` rows as `NA`, so `compute_percent_reduction()` will propagate missing values and the downstream decision-making is based on partial data. Since we want hard failures, drop the `try/next` path and let integration errors stop the run.
- **Hidden reliance on outer-scope `tfinal`.** The inner `simulate_dose_response()` expects a `tfinal` value but does not accept it as an argument (line 69). Instead it captures the symbol from the parent function, which is easy to break if `tfinal` is renamed or removed. Passing `tfinal` explicitly would make the interface clearer and safer.
- **Percent reduction assumes the placebo run succeeds.** `compute_percent_reduction()` takes `first(AUCV)` within each schedule (line 26), so if the lowest dose fails (or is reordered) everything becomes `NA` or mis-scaled. Once the error handling above is fixed this should be less fragile, but consider validating that the reference dose is present before computing reductions.

## fit-model-function.R
- **Objective function still guards the ODE call.** The optimizer wrapper uses `try(do.call(simulate_model, ...), silent = TRUE)` and returns `1e10` on failure (lines 74–87). This hides solver issues behind an arbitrary penalty even though the rest of the workflow assumes the calibration should abort when something goes wrong. Removing the `try` will make failures visible immediately.
- **Time matching depends on exact equality.** Predictions are extracted via `odeout[match(xvals, tvec), ]` (lines 112–125). This only works if data times fall exactly on the solver's grid. Any drift in `dt` or adaptive stepping would return `NA` predictions and, eventually, `NA` objectives. Using interpolation (e.g., `approx`) would be more robust.

## model-simulator-function.R
- **State re-assignment inside the virus clamp.** The block that forces `V` to zero for small values assigns `V <- 0` inside `with(...)` (lines 35–47). That local assignment does not feed back into the state vector unless the ODE solver revisits the callback immediately, so the following derivatives still see the pre-clamped `V`. If the intent is to clamp both the state and its derivative, consider returning the clamped value via the derivative (e.g., set `dV` based solely on the clamp) or manipulating `y` in the event function instead.
