"""
Fitting workflow (Julia equivalent of `run-fit.R`)
=================================================

Why this design differs from R
------------------------------
The R workflow is a two-stage multistart pipeline based on NLopt local
algorithms. In this Julia implementation we use a hybrid global-local approach:

1. Configurable global search (`BlackBoxOptim` DE by default; optional `NLopt` global algorithms).
2. Local bound-constrained refinement with `NLopt`.
3. Optional local restarts around the best solution.

This setup tends to explore rugged, multi-modal landscapes better than relying
only on local methods from many random starts, while still preserving a robust
local polish step.
"""

"""
Return default fitting settings.

The returned NamedTuple is intentionally explicit so users can copy this block
and tweak only a few fields without digging through implementation details.
"""
function default_fit_settings(model_choice::String = "model1")
    return (
        model_choice = model_choice,
        run_multistart_stage = true,
        run_sampling_stage = false,
        n_workers = 1, # currently used for sampling-stage threaded execution
        verbose_fit_log = true,
        log_top_n = 5,
        reuse_previous_bestfit = true,
        previous_bestfit_file = repo_path("results", "output", "$(model_choice)-bestfit-multistart.jld2"),
        previous_bestfit_n = 5,
        sigma_to_fit = String[],
        weight_mode = "equal_quantity",
        user_fixed_params = Dict{String,Float64}(),
        solver_settings = (solvertype = "rodas5p", tols = 1e-10, tfinal = 7.0, dt = 0.1),

        # Global-local optimizer settings.
        use_log_space = true,
        global_maxeval = 60_000,
        global_population = 120,
        global_nthreads = 1,
        global_trace_mode = :compact,
        global_trace_interval = 10.0,
        global_optimizer = :blackboxoptim,
        global_nlopt_algorithm = :GN_ESCH,
        global_nlopt_population = 0,
        n_local_restarts = 12,
        local_jitter_scale = 0.1,
        local_maxiters = 2_500,
        local_optimizer = :LN_BOBYQA,
        local_show_trace = false,

        # Sampling-stage settings (optional).
        nsamp = 20,
        fixed_overrides = Dict("Emax_V" => 1.0),
        sample_lower_factor = 0.5,
        sample_upper_factor = 2.0,
        sample_seed = 1234,
        sampling_use_log_space = true,
        sampling_local_maxiters = 1_000,
        sampling_local_optimizer = :LN_BOBYQA,
        sampling_local_show_trace = false,

        # Output paths.
        output_dir = repo_path("results", "output"),
    )
end

"""
Create sampled fixed-parameter sets around a baseline dictionary.

The first element is always the baseline. Additional samples are generated by
Latin Hypercube Sampling (LHS) over `[lower_factor * base, upper_factor * base]`
for each fixed parameter.
"""
function make_fixed_parameter_samples(
    fixedpars::Dict{String,Float64},
    nsamp::Int;
    seed::Int = 1234,
    lower_factor::Real = 0.5,
    upper_factor::Real = 2.0,
)
    rng = MersenneTwister(seed)
    lower_factor_f = Float64(lower_factor)
    upper_factor_f = Float64(upper_factor)
    samples = Vector{Dict{String,Float64}}()
    push!(samples, deepcopy(fixedpars))

    if nsamp <= 0
        return samples
    end

    keys_sorted = sort(collect(keys(fixedpars)))
    n_par = length(keys_sorted)

    # LHS in [0, 1]: each parameter column is stratified into nsamp bins.
    lhs_unit = Matrix{Float64}(undef, nsamp, n_par)
    for j in 1:n_par
        perm = randperm(rng, nsamp)
        for i in 1:nsamp
            lhs_unit[i, j] = (perm[i] - rand(rng)) / nsamp
        end
    end

    for i in 1:nsamp
        d = Dict{String,Float64}()
        for (j, k) in enumerate(keys_sorted)
            base = fixedpars[k]
            lo = lower_factor_f * base
            hi = upper_factor_f * base
            d[k] = lhs_unit[i, j] * (hi - lo) + lo
        end
        push!(samples, d)
    end

    return samples
end

"""
Convert named parameter dictionaries into aligned vectors and back.
"""
dict_to_vec(d::Dict{String,Float64}, names::Vector{String}) = [Float64(d[n]) for n in names]
vec_to_dict(v::AbstractVector{<:Real}, names::Vector{String}) = Dict(names[i] => Float64(v[i]) for i in eachindex(names))
fit_logging_enabled(settings) = hasproperty(settings, :verbose_fit_log) ? Bool(settings.verbose_fit_log) : true
fit_log_top_n(settings) = hasproperty(settings, :log_top_n) ? max(1, Int(settings.log_top_n)) : 5
timestamp_now() = Dates.format(now(), dateformat"yyyy-mm-dd HH:MM:SS")
elapsed_minutes(t0::Float64; digits::Int = 2) = round((time() - t0) / 60; digits = digits)

function emit_console_line(msg::String)
    println("[$(timestamp_now())] $msg")
    flush(stdout)
end

function log_stage_start(stage::String; detail::String = "")
    detail_msg = isempty(detail) ? "" : " | $(detail)"
    emit_console_line("Stage started: $(stage)$(detail_msg)")
    @info "Stage started" stage = stage started_at = timestamp_now() detail = detail
end

function log_stage_end(stage::String, t0::Float64; detail::String = "")
    elapsed_min = elapsed_minutes(t0)
    detail_msg = isempty(detail) ? "" : " | $(detail)"
    emit_console_line("Stage finished: $(stage) | elapsed_min=$(elapsed_min)$(detail_msg)")
    @info "Stage finished" stage = stage finished_at = timestamp_now() elapsed_min = elapsed_min detail = detail
end

"""
Return objective value as `Float64`, using `Inf` for missing/non-finite values.
"""
function bestfit_objective_or_inf(bestfit::AbstractDict)
    haskey(bestfit, "objective") || return Inf
    obj = try
        Float64(bestfit["objective"])
    catch
        Inf
    end
    return isfinite(obj) ? obj : Inf
end

"""
Sort bestfit objects by objective from best to worst.
"""
function sort_bestfits_by_objective!(bestfits::Vector{Dict{String,Any}})
    sort!(bestfits, by = bestfit_objective_or_inf)
    return bestfits
end

"""
Load and validate a multistart bestfit list used to seed sampling-only runs.
"""
function load_multistart_bestfits(multistart_path::String)
    isfile(multistart_path) || error(
        "Multistart bestfit file not found at $(multistart_path). Run with run_multistart_stage = true at least once first.",
    )

    obj = load_julia_object(multistart_path)
    obj isa AbstractVector || error("Multistart bestfit file must contain a vector of bestfit objects.")
    length(obj) > 0 || error("Multistart bestfit file is empty; cannot seed sampling stage.")

    bestfits = Vector{Dict{String,Any}}()
    for item in obj
        item isa AbstractDict || continue
        push!(bestfits, Dict{String,Any}(String(k) => v for (k, v) in pairs(item)))
    end
    isempty(bestfits) && error("Multistart bestfit file has no readable bestfit objects.")

    sort_bestfits_by_objective!(bestfits)
    return bestfits
end

"""
Extract a limited number of reusable start vectors from a prior bestfit file.

Only fit objects that contain all current `fitpar_names` are retained.
"""
function load_previous_start_vectors(
    previous_file::String,
    fitpar_names::Vector{String};
    max_count::Int = 5,
)
    starts = Vector{Vector{Float64}}()
    isfile(previous_file) || return starts

    obj = try
        load_julia_object(previous_file)
    catch
        return starts
    end
    obj isa AbstractVector || return starts

    max_count = max(0, max_count)
    for item in obj
        length(starts) >= max_count && break
        item isa AbstractDict || continue
        haskey(item, "fitpars") || continue

        fp_raw = item["fitpars"]
        fp = try
            Dict{String,Float64}(String(k) => Float64(v) for (k, v) in pairs(fp_raw))
        catch
            continue
        end

        all(haskey(fp, n) for n in fitpar_names) || continue
        x = [Float64(fp[n]) for n in fitpar_names]
        any(!isfinite, x) && continue
        push!(starts, x)
    end

    return starts
end

"""
Run bounded local refinement from one start point.
"""
function local_refine(
    x0::Vector{Float64},
    lb::Vector{Float64},
    ub::Vector{Float64},
    objective_fn::Function,
    maxiters::Int,
    optimizer_kind::Symbol = :LN_BOBYQA,
    show_trace::Bool = false,
)
    xstart = clamp.(x0, lb, ub)
    algo = optimizer_kind
    npars = length(xstart)

    opt = NLopt.Opt(algo, npars)
    NLopt.lower_bounds!(opt, lb)
    NLopt.upper_bounds!(opt, ub)
    NLopt.maxeval!(opt, maxiters)
    NLopt.xtol_rel!(opt, 1e-8)

    span = max.(ub .- lb, 1e-8)
    initial_step = max.(1e-8, 0.05 .* span)
    NLopt.initial_step!(opt, initial_step)

    eval_count = Ref(0)
    best_seen = Ref(Inf)
    NLopt.min_objective!(
        opt,
        function(x, grad)
            eval_count[] += 1
            if !isempty(grad)
                fill!(grad, 0.0)
            end
            val = objective_fn(Float64.(x))
            val_f = isfinite(val) ? Float64(val) : 1e10
            if val_f < best_seen[]
                best_seen[] = val_f
            end
            if show_trace && (eval_count[] == 1 || eval_count[] % 25 == 0)
                emit_console_line(
                    "NLopt local trace: eval=$(eval_count[]), objective=$(round(val_f; sigdigits = 8)), best=$(round(best_seen[]; sigdigits = 8))",
                )
            end
            return val_f
        end,
    )

    minf, minx, ret = NLopt.optimize(opt, xstart)

    x_min = clamp.(Float64.(minx), lb, ub)
    obj_min = objective_fn(x_min)
    if !isfinite(obj_min)
        obj_min = Float64(minf)
    end
    return (
        x = x_min,
        objective = Float64(obj_min),
        iterations = Int(NLopt.numevals(opt)),
    )
end

"""
Run global Differential Evolution search using BlackBoxOptim.
"""
function global_search_de(
    objective_fn::Function,
    lb::Vector{Float64},
    ub::Vector{Float64};
    maxeval::Int,
    population::Int,
    nthreads::Int = 1,
    trace_mode::Symbol = :silent,
    trace_interval::Float64 = 10.0,
    kwargs...,
)
    ranges = [(lb[i], ub[i]) for i in eachindex(lb)]
    nthreads_eff = max(1, Int(nthreads))
    if nthreads_eff > Threads.nthreads()
        @warn "Requested global_nthreads exceeds available Julia threads; clamping to available." requested = nthreads_eff available = Threads.nthreads()
        nthreads_eff = Threads.nthreads()
    end
    if nthreads_eff > 1
        res = bboptimize(
            objective_fn;
            SearchRange = ranges,
            NumDimensions = length(lb),
            Method = :adaptive_de_rand_1_bin_radiuslimited,
            MaxFuncEvals = maxeval,
            PopulationSize = population,
            NThreads = nthreads_eff,
            TraceMode = trace_mode,
            TraceInterval = trace_interval,
        )
    else
        res = bboptimize(
            objective_fn;
            SearchRange = ranges,
            NumDimensions = length(lb),
            Method = :adaptive_de_rand_1_bin_radiuslimited,
            MaxFuncEvals = maxeval,
            PopulationSize = population,
            TraceMode = trace_mode,
            TraceInterval = trace_interval,
        )
    end
    return (x = Float64.(best_candidate(res)), objective = Float64(best_fitness(res)))
end

global_search_blackboxoptim(args...; kwargs...) = global_search_de(args...; kwargs...)
global_search_blackbox(args...; kwargs...) = global_search_de(args...; kwargs...)
global_search_bbo(args...; kwargs...) = global_search_de(args...; kwargs...)

"""
Run global search using NLopt global algorithms.
"""
function global_search_nlopt(
    objective_fn::Function,
    lb::Vector{Float64},
    ub::Vector{Float64};
    maxeval::Int,
    algorithm::Symbol = :GN_ESCH,
    nlopt_population::Int = 0,
    nthreads::Int = 1,
    trace_mode::Symbol = :silent,
    trace_interval::Float64 = 10.0,
    kwargs...,
)
    algo = algorithm
    npars = length(lb)
    opt = NLopt.Opt(algo, npars)
    NLopt.lower_bounds!(opt, lb)
    NLopt.upper_bounds!(opt, ub)
    NLopt.maxeval!(opt, maxeval)
    NLopt.xtol_rel!(opt, 1e-8)

    NLopt.population!(opt, Int(nlopt_population))

    do_trace = Symbol(trace_mode) != :silent
    eval_count = Ref(0)
    best_seen = Ref(Inf)
    t_last_trace = Ref(time())
    NLopt.min_objective!(
        opt,
        function(x, grad)
            eval_count[] += 1
            if !isempty(grad)
                fill!(grad, 0.0)
            end
            val = objective_fn(Float64.(x))
            val_f = isfinite(val) ? Float64(val) : 1e10
            if val_f < best_seen[]
                best_seen[] = val_f
            end
            if do_trace
                now_t = time()
                if (eval_count[] == 1) || (now_t - t_last_trace[] >= trace_interval)
                    emit_console_line(
                        "NLopt global trace: eval=$(eval_count[]), objective=$(round(val_f; sigdigits = 8)), best=$(round(best_seen[]; sigdigits = 8))",
                    )
                    t_last_trace[] = now_t
                end
            end
            return val_f
        end,
    )

    xstart = clamp.(0.5 .* (lb .+ ub), lb, ub)
    minf, minx, ret = NLopt.optimize(opt, xstart)

    x_min = clamp.(Float64.(minx), lb, ub)
    obj_min = objective_fn(x_min)
    if !isfinite(obj_min)
        obj_min = Float64(minf)
    end

    return (x = x_min, objective = Float64(obj_min))
end

"""
Run configured global search backend for stage-1 optimization.
"""
function global_search(
    objective_fn::Function,
    lb::Vector{Float64},
    ub::Vector{Float64};
    maxeval::Int,
    population::Int,
    nthreads::Int = 1,
    trace_mode::Symbol = :silent,
    trace_interval::Float64 = 10.0,
    backend::Symbol = :blackboxoptim,
    nlopt_algorithm::Symbol = :GN_ESCH,
    nlopt_population::Int = 0,
)
    backend_fun = getfield(@__MODULE__, Symbol("global_search_" * String(backend)))
    return backend_fun(
        objective_fn,
        lb,
        ub;
        maxeval = maxeval,
        population = population,
        nlopt_population = nlopt_population,
        nthreads = nthreads,
        trace_mode = trace_mode,
        trace_interval = trace_interval,
        algorithm = nlopt_algorithm,
    )
end

"""
Build the objective closure used by global and local optimizers.

The closure accepts transformed-space vectors (`x`), converts to natural scale if
`use_log_space=true`, then evaluates `evaluate_objective_fast`.
"""
function make_objective_closure(
    model_choice::String,
    fitpar_names::Vector{String},
    fixedpars::Dict{String,Float64},
    y0::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    solver_settings,
    objective_data;
    use_log_space::Bool,
)
    return function(x)
        xv = use_log_space ? exp.(Float64.(x)) : Float64.(x)
        fitpars = vec_to_dict(xv, fitpar_names)
        val = evaluate_objective_fast(
            model_choice,
            fitpars,
            fixedpars,
            y0,
            doses,
            scenarios,
            solver_settings,
            objective_data;
            return_block_breakdown = false,
        )
        return isfinite(val) ? Float64(val) : 1e10
    end
end

"""
Create a compact bestfit object containing exactly what downstream steps need.
"""
function make_bestfit_object(
    fitpars::Dict{String,Float64},
    fitpar_names::Vector{String},
    fixedpars::Dict{String,Float64},
    y0::Dict{String,Float64},
    fitdata::DataFrame,
    parlabels::Dict{String,String},
    objective::Float64,
)
    return Dict(
        "fitpars" => fitpars,
        "fitparnames" => fitpar_names,
        "fixedpars" => fixedpars,
        "Y0" => y0,
        "fitdata" => fitdata,
        "parlabels" => parlabels,
        "objective" => objective,
    )
end

"""
Run one fit for a specific fixed-parameter set.

Returns a vector of local-refinement fits sorted by objective (best first).
"""
function fit_one_fixed_parameter_set(
    settings,
    model_config,
    fitdata::DataFrame,
    fitpar_names::Vector{String},
    par_ini::Dict{String,Float64},
    lb::Dict{String,Float64},
    ub::Dict{String,Float64},
    fixedpars::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    objective_data,
    previous_start_vectors_nat::Vector{Vector{Float64}} = Vector{Vector{Float64}}(),
    fit_label::String = "fit",
)
    verbose_log = fit_logging_enabled(settings)
    t_fit_block = time()
    fit_block_started_at = timestamp_now()

    x0_nat = dict_to_vec(par_ini, fitpar_names)
    lb_nat = dict_to_vec(lb, fitpar_names)
    ub_nat = dict_to_vec(ub, fitpar_names)

    if settings.use_log_space
        x0 = log.(x0_nat)
        lbv = log.(lb_nat)
        ubv = log.(ub_nat)
    else
        x0 = x0_nat
        lbv = lb_nat
        ubv = ub_nat
    end

    objective_fn = make_objective_closure(
        settings.model_choice,
        fitpar_names,
        fixedpars,
        model_config.y0,
        doses,
        scenarios,
        settings.solver_settings,
        objective_data;
        use_log_space = settings.use_log_space,
    )

    obj_x0 = verbose_log ? objective_fn(x0) : NaN

    t_global = time()
    global_backend = hasproperty(settings, :global_optimizer) ? Symbol(settings.global_optimizer) : :blackboxoptim
    global_nlopt_algorithm = hasproperty(settings, :global_nlopt_algorithm) ? Symbol(settings.global_nlopt_algorithm) : :GN_ESCH
    global_nlopt_population = hasproperty(settings, :global_nlopt_population) ? Int(settings.global_nlopt_population) : 0
    global_best = global_search(
        objective_fn,
        lbv,
        ubv;
        maxeval = settings.global_maxeval,
        population = settings.global_population,
        nthreads = Int(settings.global_nthreads),
        trace_mode = Symbol(settings.global_trace_mode),
        trace_interval = Float64(settings.global_trace_interval),
        backend = global_backend,
        nlopt_algorithm = global_nlopt_algorithm,
        nlopt_population = global_nlopt_population,
    )
    global_elapsed_min = elapsed_minutes(t_global)

    # Local refinement starts are ordered as:
    # 1) reused prior bestfit starts (if provided), then
    # 2) global-seeded starts: global best + jittered neighbors.
    # `settings.n_local_restarts` is the total number of starts in the global-seeded
    # block (including the global best itself).
    starts = Vector{Vector{Float64}}()

    n_reused = 0
    for x_nat in previous_start_vectors_nat
        length(x_nat) == length(fitpar_names) || continue
        if settings.use_log_space
            any(x_nat .<= 0.0) && continue
            x_prev = clamp.(log.(x_nat), lbv, ubv)
        else
            x_prev = clamp.(x_nat, lbv, ubv)
        end
        push!(starts, x_prev)
        n_reused += 1
    end

    push!(starts, copy(global_best.x))
    n_global_seeded = 1

    rng = Random.default_rng()
    span = ubv .- lbv
    jitter_scale = hasproperty(settings, :local_jitter_scale) ? Float64(settings.local_jitter_scale) : 0.1
    n_jitter = max(0, Int(settings.n_local_restarts) - 1)
    for _ in 1:n_jitter
        jitter = jitter_scale .* span .* randn(rng, length(span))
        xj = clamp.(global_best.x .+ jitter, lbv, ubv)
        push!(starts, xj)
    end

    t_local = time()
    n_starts = length(starts)
    local_results = Vector{NamedTuple{(:x, :objective, :iterations),Tuple{Vector{Float64},Float64,Int}}}(undef, n_starts)
    local_start_objectives = Vector{Float64}(undef, n_starts)
    local_final_objectives = Vector{Float64}(undef, n_starts)
    emit_console_line(
        "Stage 2 local refinement starts: reused=$(n_reused), global_best=$(n_global_seeded), jittered=$(n_jitter), total=$(length(starts)), jitter_scale=$(jitter_scale).",
    )
    emit_console_line("Stage 2 local refinement: running $(length(starts)) local starts.")
    use_threading = settings.n_workers > 1 && Threads.nthreads() > 1 && n_starts > 1
    if use_threading
        max_workers = min(max(1, Int(settings.n_workers)), Threads.nthreads())
        @info "Stage 2 local refinement using threaded execution" requested_workers = settings.n_workers active_workers = max_workers available_threads = Threads.nthreads()
        sem = Base.Semaphore(max_workers)
        tasks = Vector{Task}(undef, n_starts)

        for (i, s) in enumerate(starts)
            Base.acquire(sem)
            tasks[i] = Threads.@spawn begin
                try
                    start_obj = objective_fn(s)
                    local_start_objectives[i] = start_obj
                    emit_console_line("Stage 2 local refine $(i)/$(n_starts) start objective = $(round(start_obj; sigdigits = 8))")

                    lr = local_refine(
                        s,
                        lbv,
                        ubv,
                        objective_fn,
                        settings.local_maxiters,
                        Symbol(settings.local_optimizer),
                        Bool(settings.local_show_trace),
                    )

                    local_final_objectives[i] = lr.objective
                    local_results[i] = lr
                    emit_console_line("Stage 2 local refine $(i)/$(n_starts) final objective = $(round(lr.objective; sigdigits = 8))")
                finally
                    Base.release(sem)
                end
            end
        end

        for t in tasks
            fetch(t)
        end
    else
        for (i, s) in enumerate(starts)
            start_obj = objective_fn(s)
            local_start_objectives[i] = start_obj
            emit_console_line("Stage 2 local refine $(i)/$(n_starts) start objective = $(round(start_obj; sigdigits = 8))")

            lr = local_refine(
                s,
                lbv,
                ubv,
                objective_fn,
                settings.local_maxiters,
                Symbol(settings.local_optimizer),
                Bool(settings.local_show_trace),
            )

            local_final_objectives[i] = lr.objective
            local_results[i] = lr
            emit_console_line("Stage 2 local refine $(i)/$(n_starts) final objective = $(round(lr.objective; sigdigits = 8))")
        end
    end
    emit_console_line("Stage 2 local refinement start objectives ($(length(local_start_objectives))) = $(local_start_objectives)")
    emit_console_line("Stage 2 local refinement final objectives ($(length(local_final_objectives))) = $(local_final_objectives)")
    local_elapsed_min = elapsed_minutes(t_local)

    sort!(local_results, by = x -> x.objective)
    if verbose_log
        top_n = min(fit_log_top_n(settings), length(local_results))
        top_vals = [local_results[i].objective for i in 1:top_n]
        procedure_label = "global_$(String(global_backend))_then_local_refine"
        @info "Fit block summary" label = fit_label started_at = fit_block_started_at finished_at = timestamp_now() procedure = procedure_label global_backend = global_backend global_nlopt_algorithm = global_nlopt_algorithm objective_initial = obj_x0 objective_global = global_best.objective objective_best = local_results[1].objective n_local_starts = length(starts) n_reused_prior_starts = n_reused global_elapsed_min = global_elapsed_min local_elapsed_min = local_elapsed_min total_elapsed_min = elapsed_minutes(t_fit_block) local_start_objectives = local_start_objectives local_final_objectives = local_final_objectives top_n = top_n top_objectives = top_vals
    end

    # Convert to compact bestfit objects.
    out = Vector{Dict{String,Any}}()
    for lr in local_results
        x_nat = settings.use_log_space ? exp.(lr.x) : lr.x
        fitpars = vec_to_dict(x_nat, fitpar_names)
        parlabels = Dict(k => get(model_config.parlabels_full, k, k) for k in fitpar_names)
        push!(out, make_bestfit_object(
            fitpars,
            fitpar_names,
            fixedpars,
            model_config.y0,
            fitdata,
            parlabels,
            lr.objective,
        ))
    end

    return out
end

"""
Run one local-only fit for a fixed-parameter sample.

This function is used in the sampling stage and intentionally skips global DE.
It runs a single bounded local refinement started from `start_fitpars_nat`
(typically the baseline best-fit parameters from stage 1), and returns the
bestfit object plus concise run metrics for logging.
"""
function fit_one_fixed_parameter_set_local_only(
    settings,
    model_config,
    fitdata::DataFrame,
    fitpar_names::Vector{String},
    par_ini::Dict{String,Float64},
    lb::Dict{String,Float64},
    ub::Dict{String,Float64},
    fixedpars::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    objective_data,
    start_fitpars_nat::Dict{String,Float64},
    fit_label::String = "sample_local",
)
    # Build natural-scale start vector from stage-1 best fit where available;
    # fallback to par_ini for any missing entries.
    x0_nat = [haskey(start_fitpars_nat, n) ? Float64(start_fitpars_nat[n]) : Float64(par_ini[n]) for n in fitpar_names]
    lb_nat = dict_to_vec(lb, fitpar_names)
    ub_nat = dict_to_vec(ub, fitpar_names)

    use_log = Bool(settings.sampling_use_log_space)
    if use_log
        # Keep starts inside model bounds; avoid forcing tiny valid parameters
        # (e.g., 1e-14) up to an arbitrary floor.
        x0_nat = clamp.(x0_nat, lb_nat, ub_nat)
        x0_nat = max.(x0_nat, nextfloat(0.0))
        x0 = log.(x0_nat)
        lbv = log.(lb_nat)
        ubv = log.(ub_nat)
    else
        x0 = x0_nat
        lbv = lb_nat
        ubv = ub_nat
    end

    objective_fn = make_objective_closure(
        settings.model_choice,
        fitpar_names,
        fixedpars,
        model_config.y0,
        doses,
        scenarios,
        settings.solver_settings,
        objective_data;
        use_log_space = use_log,
    )

    obj_x0 = objective_fn(x0)

    lr = local_refine(
        x0,
        lbv,
        ubv,
        objective_fn,
        Int(settings.sampling_local_maxiters),
        Symbol(settings.sampling_local_optimizer),
        Bool(settings.sampling_local_show_trace),
    )

    x_nat = use_log ? exp.(lr.x) : lr.x
    fitpars = vec_to_dict(x_nat, fitpar_names)
    parlabels = Dict(k => get(model_config.parlabels_full, k, k) for k in fitpar_names)
    out = make_bestfit_object(
        fitpars,
        fitpar_names,
        fixedpars,
        model_config.y0,
        fitdata,
        parlabels,
        lr.objective,
    )

    return (
        bestfit = out,
        objective_initial = obj_x0,
        objective_final = lr.objective,
        steps = lr.iterations,
    )
end

"""
Main entry point: run fitting workflow and save outputs.

Saved files
-----------
- `<model>-bestfit-multistart.jld2`
- `<model>-bestfit-sample.jld2` (if sampling stage enabled)
"""
function run_fit_workflow(settings = default_fit_settings())
    start_wall = now()
    t_run = time()
    run_multistart_stage = hasproperty(settings, :run_multistart_stage) ? Bool(settings.run_multistart_stage) : true
    run_sampling_stage = hasproperty(settings, :run_sampling_stage) ? Bool(settings.run_sampling_stage) : false
    run_multistart_stage || run_sampling_stage || error(
        "At least one stage must be enabled: run_multistart_stage or run_sampling_stage.",
    )

    @info "Starting Julia run-fit" model = settings.model_choice started_at = timestamp_now()
    global_optimizer = hasproperty(settings, :global_optimizer) ? Symbol(settings.global_optimizer) : :blackboxoptim
    global_nlopt_algorithm = hasproperty(settings, :global_nlopt_algorithm) ? Symbol(settings.global_nlopt_algorithm) : :GN_ESCH
    global_nlopt_population = hasproperty(settings, :global_nlopt_population) ? Int(settings.global_nlopt_population) : 0
    @info "Fit settings" run_multistart_stage = run_multistart_stage run_sampling_stage = run_sampling_stage n_workers = settings.n_workers use_log_space = settings.use_log_space weight_mode = settings.weight_mode sigma_to_fit = join(settings.sigma_to_fit, ", ") solver_settings = settings.solver_settings global_optimizer = global_optimizer global_nlopt_algorithm = global_nlopt_algorithm global_nlopt_population = global_nlopt_population global_maxeval = settings.global_maxeval global_population = settings.global_population global_nthreads = settings.global_nthreads global_trace_mode = settings.global_trace_mode global_trace_interval = settings.global_trace_interval n_local_restarts = settings.n_local_restarts local_jitter_scale = settings.local_jitter_scale local_maxiters = settings.local_maxiters local_optimizer = settings.local_optimizer local_show_trace = settings.local_show_trace nsamp = settings.nsamp sampling_use_log_space = settings.sampling_use_log_space sampling_local_maxiters = settings.sampling_local_maxiters sampling_local_optimizer = settings.sampling_local_optimizer sampling_local_show_trace = settings.sampling_local_show_trace reuse_previous_bestfit = settings.reuse_previous_bestfit previous_bestfit_file = settings.previous_bestfit_file previous_bestfit_n = settings.previous_bestfit_n output_dir = settings.output_dir

    setup_elapsed_min = nothing
    baseline_fit_elapsed_min = nothing
    save_outputs_elapsed_min = nothing
    sampling_elapsed_min = nothing

    t_setup = time()
    log_stage_start(
        "1/4 data + configuration setup";
        detail = "Load processed fitting data, build model configuration, prepare objective data, and assemble fit/fixed parameter sets.",
    )
    fit_bundle = load_fit_data()
    fitdata = fit_bundle.fitdata
    scenarios = fit_bundle.scenarios
    doses = Float64.(fit_bundle.doses)
    @info "Loaded fit data" n_rows = nrow(fitdata) scenarios = scenarios doses = doses

    objective_data = prepare_fast_objective_data(fitdata; weight_mode = settings.weight_mode)
    model_config = build_model_config(settings.model_choice)
    @info "Model configuration ready" model = settings.model_choice fixedpars_file = model_config.fixedpars_file

    sigma = compute_sigma_settings(fitdata; sigma_to_fit = settings.sigma_to_fit)
    @info "Sigma configuration" n_sigma_fit = length(sigma.sigma_fit_ini) n_sigma_fixed = length(sigma.sigma_fixed)

    par_ini = deepcopy(model_config.par_ini_full)
    lb = deepcopy(model_config.lb)
    ub = deepcopy(model_config.ub)

    for (k, v) in sigma.sigma_fit_ini
        par_ini[k] = v
        lb[k] = 1e-6
        ub[k] = 1e3
    end

    # Optional user-fixed params are removed from fitted vector and copied to fixed set.
    for (k, _) in settings.user_fixed_params
        pop!(par_ini, k, nothing)
        pop!(lb, k, nothing)
        pop!(ub, k, nothing)
    end

    fitpar_names = [name for name in model_config.fitpar_order if haskey(par_ini, name)]
    sigma_fit_names = sort(collect(keys(sigma.sigma_fit_ini)))
    append!(fitpar_names, sigma_fit_names)
    @info "Prepared fitting parameter vector" n_fitpars = length(fitpar_names) fitpar_names = fitpar_names

    previous_start_vectors_nat = Vector{Vector{Float64}}()
    if settings.reuse_previous_bestfit
        previous_start_vectors_nat = load_previous_start_vectors(
            String(settings.previous_bestfit_file),
            fitpar_names;
            max_count = Int(settings.previous_bestfit_n),
        )
        @info "Previous bestfit reuse" enabled = true file = settings.previous_bestfit_file requested = settings.previous_bestfit_n loaded = length(previous_start_vectors_nat)
    else
        @info "Previous bestfit reuse" enabled = false
    end

    fixed_bundle = load_fixed_parameters(model_config.fixedpars_file)
    fixedpars = deepcopy(fixed_bundle.values)
    for (k, v) in settings.user_fixed_params
        fixedpars[k] = v
    end
    for (k, v) in sigma.sigma_fixed
        fixedpars[k] = v
    end
    log_stage_end("1/4 data + configuration setup", t_setup)
    setup_elapsed_min = elapsed_minutes(t_setup)

    multistart_path = joinpath(settings.output_dir, "$(settings.model_choice)-bestfit-multistart.jld2")
    baseline_start_fitpars_nat = Dict{String,Float64}()

    if run_multistart_stage
        # Stage 1: baseline fit over baseline fixed-parameter set.
        stage1_t0 = time()
        log_stage_start(
            "2/4 baseline fitting";
            detail = "Run configured global search backend, then bounded local refinement from multiple starts.",
        )
        bestfits_multistart = fit_one_fixed_parameter_set(
            settings,
            model_config,
            fitdata,
            fitpar_names,
            par_ini,
            lb,
            ub,
            fixedpars,
            doses,
            scenarios,
            objective_data,
            previous_start_vectors_nat,
            "baseline",
        )
        sort_bestfits_by_objective!(bestfits_multistart)
        best_obj_baseline = bestfit_objective_or_inf(bestfits_multistart[1])
        baseline_start_fitpars_nat = Dict{String,Float64}(String(k) => Float64(v) for (k, v) in pairs(bestfits_multistart[1]["fitpars"]))
        log_stage_end(
            "2/4 baseline fitting",
            stage1_t0;
            detail = "best_objective=$(best_obj_baseline)",
        )
        baseline_fit_elapsed_min = elapsed_minutes(stage1_t0)

        t_save_baseline = time()
        log_stage_start("3/4 save baseline outputs"; detail = "Write <model>-bestfit-multistart.jld2 to results/output.")
        mkpath(settings.output_dir)
        save_julia_object(multistart_path, bestfits_multistart)
        @info "Saved multistart-equivalent fits" path = multistart_path best_objective = best_obj_baseline n_fits = length(bestfits_multistart)
        log_stage_end("3/4 save baseline outputs", t_save_baseline)
        save_outputs_elapsed_min = elapsed_minutes(t_save_baseline)
    else
        @info "2/4 baseline fitting skipped" skipped_at = timestamp_now() reason = "run_multistart_stage = false"
        @info "3/4 save baseline outputs skipped" skipped_at = timestamp_now() reason = "run_multistart_stage = false"
    end

    # Optional stage 2: fixed-parameter sampling.
    if run_sampling_stage
        if isempty(baseline_start_fitpars_nat)
            multistart_bestfits = load_multistart_bestfits(multistart_path)
            baseline_start_fitpars_nat = Dict{String,Float64}(String(k) => Float64(v) for (k, v) in pairs(multistart_bestfits[1]["fitpars"]))
            @info "Sampling seed loaded from existing multistart output" path = multistart_path objective = bestfit_objective_or_inf(multistart_bestfits[1])
        else
            @info "Sampling seed loaded from current multistart run" path = multistart_path
        end

        stage2_t0 = time()
        log_stage_start(
            "4/4 sampling stage";
            detail = "Create fixed-parameter samples and run one local-only fit per sample (seeded from baseline best fit).",
        )
        @info "Sampling settings" nsamp = settings.nsamp lower_factor = settings.sample_lower_factor upper_factor = settings.sample_upper_factor sample_seed = settings.sample_seed sampling_use_log_space = settings.sampling_use_log_space sampling_local_optimizer = settings.sampling_local_optimizer sampling_local_maxiters = settings.sampling_local_maxiters
        fixed_samples = make_fixed_parameter_samples(
            fixedpars,
            settings.nsamp;
            seed = settings.sample_seed,
            lower_factor = settings.sample_lower_factor,
            upper_factor = settings.sample_upper_factor,
        )

        # Apply fixed overrides to every sample.
        for fs in fixed_samples
            for (k, v) in settings.fixed_overrides
                fs[k] = v
            end
        end

        ns = length(fixed_samples)
        sample_bestfits = Vector{Dict{String,Any}}(undef, ns)
        use_threading = settings.n_workers > 1 && Threads.nthreads() > 1

        if use_threading
            max_workers = min(max(1, Int(settings.n_workers)), Threads.nthreads())
            @info "Sampling stage using threaded execution" requested_workers = settings.n_workers active_workers = max_workers available_threads = Threads.nthreads()
            sem = Base.Semaphore(max_workers)
            tasks = Vector{Task}(undef, ns)

            for i in eachindex(fixed_samples)
                Base.acquire(sem)
                tasks[i] = Threads.@spawn begin
                    try
                        fset = fixed_samples[i]
                        fit_i = fit_one_fixed_parameter_set_local_only(
                            settings,
                            model_config,
                            fitdata,
                            fitpar_names,
                            par_ini,
                            lb,
                            ub,
                            fset,
                            doses,
                            scenarios,
                            objective_data,
                            baseline_start_fitpars_nat,
                            "sample_$(i)",
                        )
                        sample_bestfits[i] = fit_i.bestfit
                        emit_console_line(
                            "Stage 4 sample $(i)/$(ns): objective_initial=$(round(fit_i.objective_initial; sigdigits = 8)), objective_final=$(round(fit_i.objective_final; sigdigits = 8)), steps=$(fit_i.steps)",
                        )
                    finally
                        Base.release(sem)
                    end
                end
            end

            for t in tasks
                fetch(t)
            end
        else
            for (i, fset) in enumerate(fixed_samples)
                fit_i = fit_one_fixed_parameter_set_local_only(
                    settings,
                    model_config,
                    fitdata,
                    fitpar_names,
                    par_ini,
                    lb,
                    ub,
                    fset,
                    doses,
                    scenarios,
                    objective_data,
                    baseline_start_fitpars_nat,
                    "sample_$(i)",
                )
                sample_bestfits[i] = fit_i.bestfit
                emit_console_line(
                    "Stage 4 sample $(i)/$(ns): objective_initial=$(round(fit_i.objective_initial; sigdigits = 8)), objective_final=$(round(fit_i.objective_final; sigdigits = 8)), steps=$(fit_i.steps)",
                )
            end
        end

        sample_path = joinpath(settings.output_dir, "$(settings.model_choice)-bestfit-sample.jld2")
        save_julia_object(sample_path, sample_bestfits)
        @info "Saved sampling-stage fits" path = sample_path
        best_obj_sample = minimum(bestfit_objective_or_inf(x) for x in sample_bestfits)
        log_stage_end(
            "4/4 sampling stage",
            stage2_t0;
            detail = "best_objective=$(best_obj_sample)",
        )
        sampling_elapsed_min = elapsed_minutes(stage2_t0)
    else
        @info "4/4 sampling stage skipped" skipped_at = timestamp_now() reason = "run_sampling_stage = false"
    end

    emit_console_line("Stage timing summary (minutes):")
    emit_console_line("  1/4 data + configuration setup: $(setup_elapsed_min)")
    if isnothing(baseline_fit_elapsed_min)
        emit_console_line("  2/4 baseline fitting: skipped")
    else
        emit_console_line("  2/4 baseline fitting: $(baseline_fit_elapsed_min)")
    end
    if isnothing(save_outputs_elapsed_min)
        emit_console_line("  3/4 save baseline outputs: skipped")
    else
        emit_console_line("  3/4 save baseline outputs: $(save_outputs_elapsed_min)")
    end
    if isnothing(sampling_elapsed_min)
        emit_console_line("  4/4 sampling stage: skipped")
    else
        emit_console_line("  4/4 sampling stage: $(sampling_elapsed_min)")
    end
    @info "Stage timing summary (minutes)" run_multistart_stage = run_multistart_stage run_sampling_stage = run_sampling_stage setup_elapsed_min = setup_elapsed_min baseline_fit_elapsed_min = baseline_fit_elapsed_min save_outputs_elapsed_min = save_outputs_elapsed_min sampling_elapsed_min = sampling_elapsed_min

    elapsed_min = elapsed_minutes(t_run)
    elapsed = now() - start_wall
    @info "Julia run-fit finished" finished_at = timestamp_now() elapsed_min = elapsed_min elapsed = string(elapsed)
    return nothing
end
