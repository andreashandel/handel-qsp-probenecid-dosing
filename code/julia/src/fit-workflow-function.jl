"""
Fitting workflow (Julia equivalent of `run-fit.R`)
=================================================

Why this design differs from R
------------------------------
The R workflow is a two-stage multistart pipeline based on NLopt local
algorithms. In this Julia implementation we use a hybrid global-local approach:

1. Global search with `BlackBoxOptim` Differential Evolution (DE).
2. Local bound-constrained refinement with `Optim` (`Fminbox + NelderMead`).
3. Optional local restarts around the best solution.

This setup tends to explore rugged, multi-modal landscapes better than relying
only on local methods from many random starts, while still preserving a robust
local polish step.
"""

"""
Return default fitting settings.

The returned NamedTuple is intentionally explicit so users can copy this block
and tweak only a few fields without digging through implementation details.
"""
function default_fit_settings(model_choice::String = "model1")
    return (
        model_choice = model_choice,
        run_sampling_stage = false,
        n_workers = 1, # currently used for sampling-stage threaded execution
        verbose_fit_log = true,
        log_top_n = 5,
        reuse_previous_bestfit = true,
        previous_bestfit_file = repo_path("results", "output", "$(model_choice)-bestfit-multistart.jld2"),
        previous_bestfit_n = 5,
        sigma_to_fit = String[],
        weight_mode = "equal_quantity",
        user_fixed_params = Dict{String,Float64}(),
        solver_settings = (solvertype = "rodas5p", tols = 1e-10, tfinal = 7.0, dt = 0.1),

        # Global-local optimizer settings.
        use_log_space = true,
        global_maxeval = 60_000,
        global_population = 120,
        global_nthreads = 1,
        global_trace_mode = :compact,
        global_trace_interval = 10.0,
        n_local_restarts = 12,
        local_jitter_scale = 0.1,
        local_maxiters = 2_500,
        local_optimizer = :nelder_mead,
        local_show_trace = false,

        # Sampling-stage settings (optional).
        nsamp = 20,
        fixed_overrides = Dict("Emax_V" => 1.0),
        sample_lower_factor = 0.5,
        sample_upper_factor = 2.0,
        sample_seed = 1234,
        sampling_use_log_space = true,
        sampling_local_maxiters = 1_000,
        sampling_local_optimizer = :nelder_mead,
        sampling_local_show_trace = false,

        # Output paths.
        output_dir = repo_path("results", "output"),
    )
end

"""
Create sampled fixed-parameter sets around a baseline dictionary.

The first element is always the baseline. Additional samples are generated by
Latin Hypercube Sampling (LHS) over `[lower_factor * base, upper_factor * base]`
for each fixed parameter.
"""
function make_fixed_parameter_samples(
    fixedpars::Dict{String,Float64},
    nsamp::Int;
    seed::Int = 1234,
    lower_factor::Float64 = 0.5,
    upper_factor::Float64 = 2.0,
)
    rng = MersenneTwister(seed)
    samples = Vector{Dict{String,Float64}}()
    push!(samples, deepcopy(fixedpars))

    if nsamp <= 0
        return samples
    end

    keys_sorted = sort(collect(keys(fixedpars)))
    n_par = length(keys_sorted)

    # LHS in [0, 1]: each parameter column is stratified into nsamp bins.
    lhs_unit = Matrix{Float64}(undef, nsamp, n_par)
    for j in 1:n_par
        perm = randperm(rng, nsamp)
        for i in 1:nsamp
            lhs_unit[i, j] = (perm[i] - rand(rng)) / nsamp
        end
    end

    for i in 1:nsamp
        d = Dict{String,Float64}()
        for (j, k) in enumerate(keys_sorted)
            base = fixedpars[k]
            lo = lower_factor * base
            hi = upper_factor * base
            d[k] = lhs_unit[i, j] * (hi - lo) + lo
        end
        push!(samples, d)
    end

    return samples
end

"""
Convert named parameter dictionaries into aligned vectors and back.
"""
dict_to_vec(d::Dict{String,Float64}, names::Vector{String}) = [Float64(d[n]) for n in names]
vec_to_dict(v::AbstractVector{<:Real}, names::Vector{String}) = Dict(names[i] => Float64(v[i]) for i in eachindex(names))
fit_logging_enabled(settings) = hasproperty(settings, :verbose_fit_log) ? Bool(settings.verbose_fit_log) : true
fit_log_top_n(settings) = hasproperty(settings, :log_top_n) ? max(1, Int(settings.log_top_n)) : 5
timestamp_now() = Dates.format(now(), dateformat"yyyy-mm-dd HH:MM:SS")
elapsed_minutes(t0::Float64; digits::Int = 2) = round((time() - t0) / 60; digits = digits)

function emit_console_line(msg::String)
    println("[$(timestamp_now())] $msg")
    flush(stdout)
end

function log_stage_start(stage::String; detail::String = "")
    detail_msg = isempty(detail) ? "" : " | $(detail)"
    emit_console_line("Stage started: $(stage)$(detail_msg)")
    @info "Stage started" stage = stage started_at = timestamp_now() detail = detail
end

function log_stage_end(stage::String, t0::Float64; detail::String = "")
    elapsed_min = elapsed_minutes(t0)
    detail_msg = isempty(detail) ? "" : " | $(detail)"
    emit_console_line("Stage finished: $(stage) | elapsed_min=$(elapsed_min)$(detail_msg)")
    @info "Stage finished" stage = stage finished_at = timestamp_now() elapsed_min = elapsed_min detail = detail
end

"""
Extract a limited number of reusable start vectors from a prior bestfit file.

Only fit objects that contain all current `fitpar_names` are retained.
"""
function load_previous_start_vectors(
    previous_file::String,
    fitpar_names::Vector{String};
    max_count::Int = 5,
)
    starts = Vector{Vector{Float64}}()
    isfile(previous_file) || return starts

    obj = try
        load_julia_object(previous_file)
    catch
        return starts
    end
    obj isa AbstractVector || return starts

    max_count = max(0, max_count)
    for item in obj
        length(starts) >= max_count && break
        item isa AbstractDict || continue
        haskey(item, "fitpars") || continue

        fp_raw = item["fitpars"]
        fp = try
            Dict{String,Float64}(String(k) => Float64(v) for (k, v) in pairs(fp_raw))
        catch
            continue
        end

        all(haskey(fp, n) for n in fitpar_names) || continue
        x = [Float64(fp[n]) for n in fitpar_names]
        any(!isfinite, x) && continue
        push!(starts, x)
    end

    return starts
end

"""
Run bounded local refinement from one start point.
"""
function choose_local_optimizer(optimizer_kind::Symbol)
    k = Symbol(lowercase(String(optimizer_kind)))
    if k in (:neldermead, :nelder_mead, :nm)
        return NelderMead()
    elseif k in (:lbfgs, :l_bfgs)
        return LBFGS()
    elseif k == :bfgs
        return BFGS()
    elseif k in (:cg, :conjugate_gradient)
        return ConjugateGradient()
    else
        @warn "Unknown local optimizer. Falling back to NelderMead()." optimizer_kind = optimizer_kind
        return NelderMead()
    end
end

function local_refine(
    x0::Vector{Float64},
    lb::Vector{Float64},
    ub::Vector{Float64},
    objective_fn::Function,
    maxiters::Int,
    optimizer_kind::Symbol = :nelder_mead,
    show_trace::Bool = false,
)
    # Fminbox expects a strictly interior start. Move boundary points inward.
    span = ub .- lb
    eps = max.(1e-12, 1e-8 .* max.(1.0, abs.(span)))
    lo = lb .+ eps
    hi = ub .- eps
    interior_mask = lo .< hi
    xstart = copy(x0)
    xstart[interior_mask] .= clamp.(x0[interior_mask], lo[interior_mask], hi[interior_mask])
    xstart[.!interior_mask] .= clamp.(x0[.!interior_mask], lb[.!interior_mask], ub[.!interior_mask])

    result = optimize(
        objective_fn,
        lb,
        ub,
        xstart,
        Fminbox(choose_local_optimizer(optimizer_kind)),
        Optim.Options(iterations = maxiters, show_trace = show_trace),
    )
    return (
        x = copy(Optim.minimizer(result)),
        objective = Float64(Optim.minimum(result)),
        iterations = Int(Optim.iterations(result)),
    )
end

"""
Run global Differential Evolution search using BlackBoxOptim.
"""
function global_search_de(
    objective_fn::Function,
    lb::Vector{Float64},
    ub::Vector{Float64};
    maxeval::Int,
    population::Int,
    nthreads::Int = 1,
    trace_mode::Symbol = :silent,
    trace_interval::Float64 = 10.0,
)
    ranges = [(lb[i], ub[i]) for i in eachindex(lb)]
    nthreads_eff = max(1, Int(nthreads))
    if nthreads_eff > Threads.nthreads()
        @warn "Requested global_nthreads exceeds available Julia threads; clamping to available." requested = nthreads_eff available = Threads.nthreads()
        nthreads_eff = Threads.nthreads()
    end
    if nthreads_eff > 1
        res = bboptimize(
            objective_fn;
            SearchRange = ranges,
            NumDimensions = length(lb),
            Method = :adaptive_de_rand_1_bin_radiuslimited,
            MaxFuncEvals = maxeval,
            PopulationSize = population,
            NThreads = nthreads_eff,
            TraceMode = trace_mode,
            TraceInterval = trace_interval,
        )
    else
        res = bboptimize(
            objective_fn;
            SearchRange = ranges,
            NumDimensions = length(lb),
            Method = :adaptive_de_rand_1_bin_radiuslimited,
            MaxFuncEvals = maxeval,
            PopulationSize = population,
            TraceMode = trace_mode,
            TraceInterval = trace_interval,
        )
    end
    return (x = Float64.(best_candidate(res)), objective = Float64(best_fitness(res)))
end

"""
Build the objective closure used by global and local optimizers.

The closure accepts transformed-space vectors (`x`), converts to natural scale if
`use_log_space=true`, then evaluates `evaluate_objective_fast`.
"""
function make_objective_closure(
    model_choice::String,
    fitpar_names::Vector{String},
    fixedpars::Dict{String,Float64},
    y0::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    solver_settings,
    objective_data;
    use_log_space::Bool,
)
    return function(x)
        xv = use_log_space ? exp.(Float64.(x)) : Float64.(x)
        fitpars = vec_to_dict(xv, fitpar_names)
        val = evaluate_objective_fast(
            model_choice,
            fitpars,
            fixedpars,
            y0,
            doses,
            scenarios,
            solver_settings,
            objective_data;
            return_block_breakdown = false,
        )
        return isfinite(val) ? Float64(val) : 1e10
    end
end

"""
Create a compact bestfit object containing exactly what downstream steps need.
"""
function make_bestfit_object(
    fitpars::Dict{String,Float64},
    fitpar_names::Vector{String},
    fixedpars::Dict{String,Float64},
    y0::Dict{String,Float64},
    fitdata::DataFrame,
    parlabels::Dict{String,String},
    objective::Float64,
)
    return Dict(
        "fitpars" => fitpars,
        "fitparnames" => fitpar_names,
        "fixedpars" => fixedpars,
        "Y0" => y0,
        "fitdata" => fitdata,
        "parlabels" => parlabels,
        "objective" => objective,
    )
end

"""
Run one fit for a specific fixed-parameter set.

Returns a vector of local-refinement fits sorted by objective (best first).
"""
function fit_one_fixed_parameter_set(
    settings,
    model_config,
    fitdata::DataFrame,
    fitpar_names::Vector{String},
    par_ini::Dict{String,Float64},
    lb::Dict{String,Float64},
    ub::Dict{String,Float64},
    fixedpars::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    objective_data,
    previous_start_vectors_nat::Vector{Vector{Float64}} = Vector{Vector{Float64}}(),
    fit_label::String = "fit",
)
    verbose_log = fit_logging_enabled(settings)
    t_fit_block = time()
    fit_block_started_at = timestamp_now()

    x0_nat = dict_to_vec(par_ini, fitpar_names)
    lb_nat = dict_to_vec(lb, fitpar_names)
    ub_nat = dict_to_vec(ub, fitpar_names)

    if settings.use_log_space
        x0 = log.(x0_nat)
        lbv = log.(lb_nat)
        ubv = log.(ub_nat)
    else
        x0 = x0_nat
        lbv = lb_nat
        ubv = ub_nat
    end

    objective_fn = make_objective_closure(
        settings.model_choice,
        fitpar_names,
        fixedpars,
        model_config.y0,
        doses,
        scenarios,
        settings.solver_settings,
        objective_data;
        use_log_space = settings.use_log_space,
    )

    obj_x0 = verbose_log ? objective_fn(x0) : NaN

    t_global = time()
    global_best = global_search_de(
        objective_fn,
        lbv,
        ubv;
        maxeval = settings.global_maxeval,
        population = settings.global_population,
        nthreads = Int(settings.global_nthreads),
        trace_mode = Symbol(settings.global_trace_mode),
        trace_interval = Float64(settings.global_trace_interval),
    )
    global_elapsed_min = elapsed_minutes(t_global)

    # Local refinement starts are ordered as:
    # 1) reused prior bestfit starts (if provided), then
    # 2) DE-seeded starts: global best + jittered neighbors.
    # `settings.n_local_restarts` is the total number of starts in the DE-seeded
    # block (including the global best itself).
    starts = Vector{Vector{Float64}}()

    n_reused = 0
    for x_nat in previous_start_vectors_nat
        length(x_nat) == length(fitpar_names) || continue
        if settings.use_log_space
            any(x_nat .<= 0.0) && continue
            x_prev = clamp.(log.(x_nat), lbv, ubv)
        else
            x_prev = clamp.(x_nat, lbv, ubv)
        end
        push!(starts, x_prev)
        n_reused += 1
    end

    push!(starts, copy(global_best.x))
    n_de_seeded = 1

    rng = Random.default_rng()
    span = ubv .- lbv
    jitter_scale = hasproperty(settings, :local_jitter_scale) ? Float64(settings.local_jitter_scale) : 0.1
    n_jitter = max(0, Int(settings.n_local_restarts) - 1)
    for _ in 1:n_jitter
        jitter = jitter_scale .* span .* randn(rng, length(span))
        xj = clamp.(global_best.x .+ jitter, lbv, ubv)
        push!(starts, xj)
    end

    t_local = time()
    local_results = Vector{NamedTuple{(:x, :objective, :iterations),Tuple{Vector{Float64},Float64,Int}}}()
    local_start_objectives = Float64[]
    local_final_objectives = Float64[]
    emit_console_line(
        "Stage 2 local refinement starts: reused=$(n_reused), de_best=$(n_de_seeded), jittered=$(n_jitter), total=$(length(starts)), jitter_scale=$(jitter_scale).",
    )
    emit_console_line("Stage 2 local refinement: running $(length(starts)) local starts.")
    for (i, s) in enumerate(starts)
        start_obj = objective_fn(s)
        push!(local_start_objectives, start_obj)
        emit_console_line("Stage 2 local refine $(i)/$(length(starts)) start objective = $(round(start_obj; sigdigits = 8))")

        lr = local_refine(
            s,
            lbv,
            ubv,
            objective_fn,
            settings.local_maxiters,
            Symbol(settings.local_optimizer),
            Bool(settings.local_show_trace),
        )

        push!(local_final_objectives, lr.objective)
        emit_console_line("Stage 2 local refine $(i)/$(length(starts)) final objective = $(round(lr.objective; sigdigits = 8))")
        push!(local_results, lr)
    end
    emit_console_line("Stage 2 local refinement start objectives ($(length(local_start_objectives))) = $(local_start_objectives)")
    emit_console_line("Stage 2 local refinement final objectives ($(length(local_final_objectives))) = $(local_final_objectives)")
    local_elapsed_min = elapsed_minutes(t_local)

    sort!(local_results, by = x -> x.objective)
    if verbose_log
        top_n = min(fit_log_top_n(settings), length(local_results))
        top_vals = [local_results[i].objective for i in 1:top_n]
        @info "Fit block summary" label = fit_label started_at = fit_block_started_at finished_at = timestamp_now() procedure = "global_DE_then_local_refine" objective_initial = obj_x0 objective_global = global_best.objective objective_best = local_results[1].objective n_local_starts = length(starts) n_reused_prior_starts = n_reused global_elapsed_min = global_elapsed_min local_elapsed_min = local_elapsed_min total_elapsed_min = elapsed_minutes(t_fit_block) local_start_objectives = local_start_objectives local_final_objectives = local_final_objectives top_n = top_n top_objectives = top_vals
    end

    # Convert to compact bestfit objects.
    out = Vector{Dict{String,Any}}()
    for lr in local_results
        x_nat = settings.use_log_space ? exp.(lr.x) : lr.x
        fitpars = vec_to_dict(x_nat, fitpar_names)
        parlabels = Dict(k => get(model_config.parlabels_full, k, k) for k in fitpar_names)
        push!(out, make_bestfit_object(
            fitpars,
            fitpar_names,
            fixedpars,
            model_config.y0,
            fitdata,
            parlabels,
            lr.objective,
        ))
    end

    return out
end

"""
Run one local-only fit for a fixed-parameter sample.

This function is used in the sampling stage and intentionally skips global DE.
It runs a single bounded local refinement started from `start_fitpars_nat`
(typically the baseline best-fit parameters from stage 1), and returns the
bestfit object plus concise run metrics for logging.
"""
function fit_one_fixed_parameter_set_local_only(
    settings,
    model_config,
    fitdata::DataFrame,
    fitpar_names::Vector{String},
    par_ini::Dict{String,Float64},
    lb::Dict{String,Float64},
    ub::Dict{String,Float64},
    fixedpars::Dict{String,Float64},
    doses::Vector{Float64},
    scenarios::Vector{String},
    objective_data,
    start_fitpars_nat::Dict{String,Float64},
    fit_label::String = "sample_local",
)
    # Build natural-scale start vector from stage-1 best fit where available;
    # fallback to par_ini for any missing entries.
    x0_nat = [haskey(start_fitpars_nat, n) ? Float64(start_fitpars_nat[n]) : Float64(par_ini[n]) for n in fitpar_names]
    lb_nat = dict_to_vec(lb, fitpar_names)
    ub_nat = dict_to_vec(ub, fitpar_names)

    use_log = Bool(settings.sampling_use_log_space)
    if use_log
        # Ensure positivity before log transform.
        x0_nat = max.(x0_nat, 1e-12)
        x0 = log.(x0_nat)
        lbv = log.(lb_nat)
        ubv = log.(ub_nat)
    else
        x0 = x0_nat
        lbv = lb_nat
        ubv = ub_nat
    end

    objective_fn = make_objective_closure(
        settings.model_choice,
        fitpar_names,
        fixedpars,
        model_config.y0,
        doses,
        scenarios,
        settings.solver_settings,
        objective_data;
        use_log_space = use_log,
    )

    obj_x0 = objective_fn(x0)

    lr = local_refine(
        x0,
        lbv,
        ubv,
        objective_fn,
        Int(settings.sampling_local_maxiters),
        Symbol(settings.sampling_local_optimizer),
        Bool(settings.sampling_local_show_trace),
    )

    x_nat = use_log ? exp.(lr.x) : lr.x
    fitpars = vec_to_dict(x_nat, fitpar_names)
    parlabels = Dict(k => get(model_config.parlabels_full, k, k) for k in fitpar_names)
    out = make_bestfit_object(
        fitpars,
        fitpar_names,
        fixedpars,
        model_config.y0,
        fitdata,
        parlabels,
        lr.objective,
    )

    return (
        bestfit = out,
        objective_initial = obj_x0,
        objective_final = lr.objective,
        steps = lr.iterations,
    )
end

"""
Main entry point: run fitting workflow and save outputs.

Saved files
-----------
- `<model>-bestfit-multistart.jld2`
- `<model>-bestfit-sample.jld2` (if sampling stage enabled)
"""
function run_fit_workflow(settings = default_fit_settings())
    start_wall = now()
    t_run = time()
    @info "Starting Julia run-fit" model = settings.model_choice started_at = timestamp_now()
    @info "Fit settings" run_sampling_stage = settings.run_sampling_stage n_workers = settings.n_workers use_log_space = settings.use_log_space weight_mode = settings.weight_mode sigma_to_fit = join(settings.sigma_to_fit, ", ") solver_settings = settings.solver_settings global_maxeval = settings.global_maxeval global_population = settings.global_population global_nthreads = settings.global_nthreads global_trace_mode = settings.global_trace_mode global_trace_interval = settings.global_trace_interval n_local_restarts = settings.n_local_restarts local_jitter_scale = settings.local_jitter_scale local_maxiters = settings.local_maxiters local_optimizer = settings.local_optimizer local_show_trace = settings.local_show_trace nsamp = settings.nsamp sampling_use_log_space = settings.sampling_use_log_space sampling_local_maxiters = settings.sampling_local_maxiters sampling_local_optimizer = settings.sampling_local_optimizer sampling_local_show_trace = settings.sampling_local_show_trace reuse_previous_bestfit = settings.reuse_previous_bestfit previous_bestfit_file = settings.previous_bestfit_file previous_bestfit_n = settings.previous_bestfit_n output_dir = settings.output_dir

    setup_elapsed_min = NaN
    baseline_fit_elapsed_min = NaN
    save_outputs_elapsed_min = NaN
    sampling_elapsed_min = nothing

    t_setup = time()
    log_stage_start(
        "1/4 data + configuration setup";
        detail = "Load processed fitting data, build model configuration, prepare objective data, and assemble fit/fixed parameter sets.",
    )
    fit_bundle = load_fit_data()
    fitdata = fit_bundle.fitdata
    scenarios = fit_bundle.scenarios
    doses = Float64.(fit_bundle.doses)
    @info "Loaded fit data" n_rows = nrow(fitdata) scenarios = scenarios doses = doses

    objective_data = prepare_fast_objective_data(fitdata; weight_mode = settings.weight_mode)
    model_config = build_model_config(settings.model_choice)
    @info "Model configuration ready" model = settings.model_choice fixedpars_file = model_config.fixedpars_file

    sigma = compute_sigma_settings(fitdata; sigma_to_fit = settings.sigma_to_fit)
    @info "Sigma configuration" n_sigma_fit = length(sigma.sigma_fit_ini) n_sigma_fixed = length(sigma.sigma_fixed)

    par_ini = deepcopy(model_config.par_ini_full)
    lb = deepcopy(model_config.lb)
    ub = deepcopy(model_config.ub)

    for (k, v) in sigma.sigma_fit_ini
        par_ini[k] = v
        lb[k] = 1e-6
        ub[k] = 1e3
    end

    # Optional user-fixed params are removed from fitted vector and copied to fixed set.
    for (k, _) in settings.user_fixed_params
        pop!(par_ini, k, nothing)
        pop!(lb, k, nothing)
        pop!(ub, k, nothing)
    end

    fitpar_names = [name for name in model_config.fitpar_order if haskey(par_ini, name)]
    sigma_fit_names = sort(collect(keys(sigma.sigma_fit_ini)))
    append!(fitpar_names, sigma_fit_names)
    @info "Prepared fitting parameter vector" n_fitpars = length(fitpar_names) fitpar_names = fitpar_names

    previous_start_vectors_nat = Vector{Vector{Float64}}()
    if settings.reuse_previous_bestfit
        previous_start_vectors_nat = load_previous_start_vectors(
            String(settings.previous_bestfit_file),
            fitpar_names;
            max_count = Int(settings.previous_bestfit_n),
        )
        @info "Previous bestfit reuse" enabled = true file = settings.previous_bestfit_file requested = settings.previous_bestfit_n loaded = length(previous_start_vectors_nat)
    else
        @info "Previous bestfit reuse" enabled = false
    end

    fixed_bundle = load_fixed_parameters(model_config.fixedpars_file)
    fixedpars = deepcopy(fixed_bundle.values)
    for (k, v) in settings.user_fixed_params
        fixedpars[k] = v
    end
    for (k, v) in sigma.sigma_fixed
        fixedpars[k] = v
    end
    log_stage_end("1/4 data + configuration setup", t_setup)
    setup_elapsed_min = elapsed_minutes(t_setup)

    # Stage 1: baseline fit over baseline fixed-parameter set.
    stage1_t0 = time()
    log_stage_start(
        "2/4 baseline fitting";
        detail = "Run global Differential Evolution search, then bounded local refinement from multiple starts.",
    )
    bestfits_multistart = fit_one_fixed_parameter_set(
        settings,
        model_config,
        fitdata,
        fitpar_names,
        par_ini,
        lb,
        ub,
        fixedpars,
        doses,
        scenarios,
        objective_data,
        previous_start_vectors_nat,
        "baseline",
    )
    best_obj_baseline = bestfits_multistart[1]["objective"]
    baseline_start_fitpars_nat = Dict{String,Float64}(String(k) => Float64(v) for (k, v) in pairs(bestfits_multistart[1]["fitpars"]))
    log_stage_end(
        "2/4 baseline fitting",
        stage1_t0;
        detail = "best_objective=$(best_obj_baseline)",
    )
    baseline_fit_elapsed_min = elapsed_minutes(stage1_t0)

    t_save_baseline = time()
    log_stage_start("3/4 save baseline outputs"; detail = "Write <model>-bestfit-multistart.jld2 to results/output.")
    mkpath(settings.output_dir)
    multistart_path = joinpath(settings.output_dir, "$(settings.model_choice)-bestfit-multistart.jld2")
    save_julia_object(multistart_path, bestfits_multistart)
    @info "Saved multistart-equivalent fits" path = multistart_path best_objective = best_obj_baseline
    log_stage_end("3/4 save baseline outputs", t_save_baseline)
    save_outputs_elapsed_min = elapsed_minutes(t_save_baseline)

    # Optional stage 2: fixed-parameter sampling.
    if settings.run_sampling_stage
        stage2_t0 = time()
        log_stage_start(
            "4/4 sampling stage";
            detail = "Create fixed-parameter samples and run one local-only fit per sample (seeded from baseline best fit).",
        )
        @info "Sampling settings" nsamp = settings.nsamp lower_factor = settings.sample_lower_factor upper_factor = settings.sample_upper_factor sample_seed = settings.sample_seed sampling_use_log_space = settings.sampling_use_log_space sampling_local_optimizer = settings.sampling_local_optimizer sampling_local_maxiters = settings.sampling_local_maxiters
        fixed_samples = make_fixed_parameter_samples(
            fixedpars,
            settings.nsamp;
            seed = settings.sample_seed,
            lower_factor = settings.sample_lower_factor,
            upper_factor = settings.sample_upper_factor,
        )

        # Apply fixed overrides to every sample.
        for fs in fixed_samples
            for (k, v) in settings.fixed_overrides
                fs[k] = v
            end
        end

        ns = length(fixed_samples)
        sample_bestfits = Vector{Dict{String,Any}}(undef, ns)
        use_threading = settings.n_workers > 1 && Threads.nthreads() > 1

        if use_threading
            max_workers = min(max(1, Int(settings.n_workers)), Threads.nthreads())
            @info "Sampling stage using threaded execution" requested_workers = settings.n_workers active_workers = max_workers available_threads = Threads.nthreads()
            sem = Base.Semaphore(max_workers)
            tasks = Vector{Task}(undef, ns)

            for i in eachindex(fixed_samples)
                Base.acquire(sem)
                tasks[i] = Threads.@spawn begin
                    try
                        fset = fixed_samples[i]
                        fit_i = fit_one_fixed_parameter_set_local_only(
                            settings,
                            model_config,
                            fitdata,
                            fitpar_names,
                            par_ini,
                            lb,
                            ub,
                            fset,
                            doses,
                            scenarios,
                            objective_data,
                            baseline_start_fitpars_nat,
                            "sample_$(i)",
                        )
                        sample_bestfits[i] = fit_i.bestfit
                        emit_console_line(
                            "Stage 4 sample $(i)/$(ns): objective_initial=$(round(fit_i.objective_initial; sigdigits = 8)), objective_final=$(round(fit_i.objective_final; sigdigits = 8)), steps=$(fit_i.steps)",
                        )
                    finally
                        Base.release(sem)
                    end
                end
            end

            for t in tasks
                fetch(t)
            end
        else
            for (i, fset) in enumerate(fixed_samples)
                fit_i = fit_one_fixed_parameter_set_local_only(
                    settings,
                    model_config,
                    fitdata,
                    fitpar_names,
                    par_ini,
                    lb,
                    ub,
                    fset,
                    doses,
                    scenarios,
                    objective_data,
                    baseline_start_fitpars_nat,
                    "sample_$(i)",
                )
                sample_bestfits[i] = fit_i.bestfit
                emit_console_line(
                    "Stage 4 sample $(i)/$(ns): objective_initial=$(round(fit_i.objective_initial; sigdigits = 8)), objective_final=$(round(fit_i.objective_final; sigdigits = 8)), steps=$(fit_i.steps)",
                )
            end
        end

        sample_path = joinpath(settings.output_dir, "$(settings.model_choice)-bestfit-sample.jld2")
        save_julia_object(sample_path, sample_bestfits)
        @info "Saved sampling-stage fits" path = sample_path
        best_obj_sample = minimum([x["objective"] for x in sample_bestfits])
        log_stage_end(
            "4/4 sampling stage",
            stage2_t0;
            detail = "best_objective=$(best_obj_sample)",
        )
        sampling_elapsed_min = elapsed_minutes(stage2_t0)
    else
        @info "4/4 sampling stage skipped" skipped_at = timestamp_now() reason = "run_sampling_stage = false"
    end

    emit_console_line("Stage timing summary (minutes):")
    emit_console_line("  1/4 data + configuration setup: $(setup_elapsed_min)")
    emit_console_line("  2/4 baseline fitting: $(baseline_fit_elapsed_min)")
    emit_console_line("  3/4 save baseline outputs: $(save_outputs_elapsed_min)")
    if isnothing(sampling_elapsed_min)
        emit_console_line("  4/4 sampling stage: skipped")
    else
        emit_console_line("  4/4 sampling stage: $(sampling_elapsed_min)")
    end
    @info "Stage timing summary (minutes)" setup_elapsed_min = setup_elapsed_min baseline_fit_elapsed_min = baseline_fit_elapsed_min save_outputs_elapsed_min = save_outputs_elapsed_min sampling_elapsed_min = sampling_elapsed_min

    elapsed_min = elapsed_minutes(t_run)
    elapsed = now() - start_wall
    @info "Julia run-fit finished" finished_at = timestamp_now() elapsed_min = elapsed_min elapsed = string(elapsed)
    return nothing
end
